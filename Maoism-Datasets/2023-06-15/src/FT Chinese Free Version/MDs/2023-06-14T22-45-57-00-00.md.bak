# 欧盟立法者将如何驯服ChatGPT们？ (Free Version)

Authors: ['']

Publisher: 英国《金融时报》

Time: 2023-06-14T22:45:57+00:00

Published Time: 2023-06-14T12:00:99+08:00

Modified Time: 2023-06-14T18:45:99+08:00

Description: 张冬方:这是一个在人工智能加持下越来越真假难辨的世界。欧盟以领先其他国家和地区的数字监管而著称，却已然跟不上人工智能发展的步伐了。

Images: ["[000183746_pic...](https://thumbor.ftacademy.cn/unsafe/picture/6/000183746_piclink.jpg)"]

Themes: ['专栏', '人工智能', '关注']

Keywords: ['人工智能', 'ChatGPT']

Type: Article

<!--METADATA-->

![](../Images/2023-06-14T22-45-57-00-00/000183746_piclink.jpg)来自FT中文网的温馨提示：如您对更多FT中文网的内容感兴趣，请在苹果应用商店或谷歌应用市场搜索“FT中文网”，下载FT中文网的官方应用。

前一段时间，美国总统拜登刚宣布将再次竞选总统，共和党全国委员会紧接着就发布了一个政治短片。短片展现了拜登连任之下美国的悲惨场景，图像逼真、情绪饱满，可其实，视频由人工智能生成。有人称，人工智能从此进入政治宣传领域。在此之前，那张警察擒拿特朗普、罗马主教身着时尚羽绒服等人工智能生成的图片已经在网络上病毒式地传播。

甚至传统大众媒体也玩起了人工智能的擦边球。一家德国杂志在其封面上刊登了已退役F1赛车手、自滑雪事故后退出公众视线的舒马赫的大幅照片，并配以“首次采访”、“全球轰动”吸人眼球的标题，采用了字体更小的“听上去实在就像真的”的模糊措辞。实际上，该杂志发表的是由人工智能生成的采访。虽然杂志内页说明了采访与人工智能有关，这起丑闻最终以主编走人、向舒马赫家人道歉、媒体公信力得到削弱而收场。

这是一个在人工智能加持下越来越真假难辨的世界。欧盟以领先其他国家和地区的数字监管而著称，却已然跟不上人工智能发展的步伐了。

欧盟的《人工智能法》（Artificial IntelligenceAct，以下简称“AIA”）正处于立法过程当中，可监管者已经等不到法律最终生效了。6月5日，欧委会副主席和欧盟内部市场专员与签署了《反虚假信息行为准则》的、包括谷歌和微软在内的科技公司代表会面，要求将生成式人工智能整合到自己应用产品当中的企业建立必要的保障措施，以防止其提供的服务被用来制造虚假内容和深度伪造，并提供能识别这些内容的技术，对其进行标记。

人工智能的发展超乎了人的想象。自2020年以来，OpenAI曾发布了GPT-3、DALL-E、ChatGPT和GPT-4，微软于今年发布了整合了AI的新版Bing搜索引擎和Edge浏览器，而谷歌则推出了以抗衡ChatGPT的聊天机器人Bard。ChatGPT的火爆打乱了欧盟人工智能立法者的节奏。

两年前，欧委会公布了AIA草案，这是一部基于人工智能风险评级方法的法律。该草案将不同的人工智能分为最低风险、低风险、高风险和不可接受的风险四个等级，比如，社会信用评分和公共空间的实时远程生物特征识别系统就属于不可接受的风险，从而位于禁止名单之列。欧委会提供的AIA最初版本，并没有覆盖到类似于DALL-E和ChatGPT的人工智能。

有研究者敲响了警钟，称基础模型（FoundationModels）这种强大的人工智能的迅速发展，有可能在宣告欧盟新立法的失效和过时。基础模型建立在庞大的数据之上，然后被应用于广泛的任务，相当于其他应用的基础设施。在研究学者看来，基础模型中的任何不足都会延续到之后的模型中。斯坦福大学的研究者2021年发布了一份关于大型语言模型的研究报告，研究者向GPT-3输入“两个穆斯林走进”这样一个未完成的句子，GPT-3完成的100个句子中有66次给出的都是暴力行为，比如“两个穆斯林走进犹太教堂，带着斧头和炸弹”，结果中的暴力内容比例远远高于当输入基督教、佛教等其他宗教时。也就是说，GPT-3呈现了将穆斯林和暴力绑定在一起的强烈偏见。研究学者认为，基础模型对这个世界并没有深刻的理解，它们收集了网络上好的、坏的、丑陋的一切，它们产生的结果并非建立在事实之上。

另外，用途的不确定意味着用途的不可控，从而意味着由此带来的风险的不可控。欧盟AIA规范的是有些某些特定用途的人工智能，而没有特定目的的基础模型有可能处于不受监管的法律真空地带。有人建议将基础模型列入法案中的“高风险”之列。

欧盟理事会在讨论草案的过程中，将通用目的人工智能（general purposeAI）的概念引入文本中，指的是没有特定目的，可以用于多种用途的人工智能。欧盟理事会在去年12月采纳的修订版中提议，草案对高风险人工智能的要求应当也适用于整合到高风险系统当中的通用目的人工智能。若通用目的人工智能系统提供者发现或被通知市场中存在对该系统的滥用行为，提供者须采取必要和恰当的措施来阻止进一步的滥用，尤其须考虑到滥用的规模和风险的严重性。这其实就是对提供者规定了监测义务，类似于《数字服务法》中通知和行动机制（noticeand action mechanism）。

相比较起欧盟成员国，欧洲议会更倾向于在AIA中增加针对生成式人工智能（generative AI）的更严格的条款。

欧洲议会议员们最担忧的就在于，如何避免生成式人工智能以声称自己没有特定目的为由，从而避开基于风险的监管。生成式人工智能可以用于好的方面，也可以用于不好的方面，不公平之处可能在于风险和责任被转移到下游，即转移到将人工智能用于高风险用途的使用者身上，而鉴于使用者出于不占有数据等原因，他们可能无法履行AIA规定的义务。

一方面，欧洲议会一度出现了将生成式人工智能统一纳入高风险系统的趋势；另一方面，Corporate EuropeObservatory今年2月发布的报告显示，包括谷歌和OpenAI最大投资方微软在内的企业锲而不舍地游说欧盟政策制定者，奋力推动将类似于ChatGPT的人工智能排除在监管之外。

欧洲议会最终达成了妥协。3月底，欧盟立法者们进行了一场技术性讨论，他们将由欧盟理事会引入的“通用目的人工智能”和源自斯坦福大学的“基础模型”这两个概念进行了区分。前者在于，可以用于和适用于广泛应用的人工智能系统，该系统的设计并非出于特定目的和应用，后者则为基于庞大的数据进行训练，用于通用性输出和适用广泛的特别任务，比如GPT-4，而后者才是欧洲议会进行更严格监管的目标。AIA欧洲议会版本对基础模型提供者提出了专门的要求，比如提供者在将基础模型投向市场之前，需对其进行测试和分析，以识别和减轻对健康、安全、基本权利和民主等方面可预见的风险。

欧洲议会尤其对生成式人工智能保持警惕。欧洲议会提议，用于生成复杂文本、图片、声音或视频的生成式人工智能的基础模型提供者还须遵循额外的义务，提供者在设计和开发基础模型时，须遵守欧盟法律和保障包括言论自由在内的基本权利，公布对受版权法保护的训练数据的使用情况等。且这些要求须贯穿到整个人工智能价值链上。尤其是，基础模型提供者协助下游系统提供者提供充分的保障措施。

有研究学者指出，欧洲议会版本中的评估、降低和管理风险仍然聚焦在模型本身，而不是具体的应用上，这只会导致风险评估愈加艰难，且用于减轻假定的风险的措施无法实现。有人提议，应该将监管的重心放到将生成式人工智能使用到高风险具体应用的使用者身上。与此同时，欧盟境内也出现另一种担忧，过度监管可能会扼杀类似于ChatGPT的人工智能系统的本土开发。

6月14日，欧洲议会投票通过了AIA谈判授权草案，接下来将进入三方协商阶段。关于生成式人工智能的争论还将持续下去。

_（本文仅代表作者本人观点，责编：闫曼 man.yan@ftchinese.com)_

Source: [http://www.ftchinese.com/story/001099964](http://www.ftchinese.com/story/001099964)